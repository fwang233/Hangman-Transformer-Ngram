{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"scrolled":true,"id":"5eblOI5etFwV"},"outputs":[],"source":["import json\n","import requests\n","import random\n","import string\n","import secrets\n","import time\n","import re\n","import collections\n","import string\n","import os\n","import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization"]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"mFV8m7_waRn-"}},{"cell_type":"code","source":["import nltk\n","nltk.download('brown')"],"metadata":{"id":"2C9lw-j8aV9a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import brown\n","import numpy as np\n","\n","np.random.seed(1)\n","\n","# training_set stores the rest word types for training\n","training_set = []\n","# test_set stores 1000 word types for testing\n","test_set = []\n","\n","#words from brown corpus\n","brown_words = brown.words()\n","\n","#lowercase the corpus and remove the word which contain non-alphabetic characters\n","processed_words = []\n","for word in brown_words:\n","    if word.isalpha():\n","        processed_words.append(word.lower())\n","\n","#unique words in brown corpus\n","unique_words = list(set(processed_words))\n","\n","# change to array in order to conduct np shuffle, then convert to list\n","unique_words = np.array(unique_words)\n","np.random.shuffle(unique_words)\n","unique_words = unique_words.tolist()"],"metadata":{"id":"U9gjZWiw2AEA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BERT Model"],"metadata":{"id":"vnznJFTjawmk"}},{"cell_type":"code","source":["loss_fn = keras.losses.SparseCategoricalCrossentropy(reduction=keras.losses.Reduction.NONE)\n","loss_tracker = keras.metrics.Mean(name=\"loss\")\n","\n","class MaskedLanguageModel(tf.keras.Model):\n","  def train_step(self, inputs):\n","    if len(inputs) == 3:\n","      features, labels, sample_weight = inputs\n","    else:\n","      features, labels = inputs\n","      sample_weight = None\n","\n","    with tf.GradientTape() as tape:\n","      predictions = self(features, training=True)\n","      loss = loss_fn(labels, predictions, sample_weight=sample_weight)\n","\n","    # Compute gradients\n","    trainable_vars = self.trainable_variables\n","    gradients = tape.gradient(loss, trainable_vars)\n","\n","    # Update weights\n","    self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n","\n","    # Compute our own metrics\n","    loss_tracker.update_state(loss, sample_weight=sample_weight)\n","\n","    # Return a dict mapping metric names to current value\n","    return {\"loss\": loss_tracker.result()}\n","\n","  @property\n","  def metrics(self):\n","      return [loss_tracker]\n","\n","class BertModel():\n","  def __init__(self, full_dictionary):\n","    self.full_dictionary = full_dictionary\n","    self.special_tokens = '.'\n","\n","    self.max_len = 30 # max length of a word\n","    self.vocab_size = 29 # number of letters plus sepcial tokens\n","    self.batch_size = 32\n","    self.lr = 0.001\n","    self.embed_dim = 29\n","    self.num_head = 8\n","    self.ff_dim = 128\n","    self.num_layers = 1\n","\n","    self.texts = self.preprocess_words()\n","    self.vectorize_layer = self.vectorize_word(self.texts)\n","    self.mask_token_id = self.get_mask_token_id(self.vectorize_layer)\n","\n","  # Reset radom seed\n","  def reset_random_seeds(self):\n","    os.environ['PYTHONHASHSEED'] = str(0)\n","    random.seed(1)\n","    np.random.seed(2)\n","    tf.random.set_seed(3)\n","\n","  # Shuffle all words\n","  def preprocess_words(self):\n","    texts = np.array(self.full_dictionary)\n","    np.random.shuffle(texts)\n","    texts = texts.tolist()\n","    return texts\n","\n","  # Build word vectorization\n","  def vectorize_word(self, texts):\n","    max_seq = self.max_len\n","\n","    vectorize_layer = TextVectorization(\n","        output_mode=\"int\",\n","        standardize=\"lower\",\n","        split=\"character\",\n","        output_sequence_length=max_seq,\n","    )\n","    vectorize_layer.adapt(texts)\n","\n","    vocab = vectorize_layer.get_vocabulary(include_special_tokens=True)\n","    vocab = vocab + [self.special_tokens]\n","    vectorize_layer.set_vocabulary(vocab)\n","    return vectorize_layer\n","\n","  # Get special token id\n","  def get_mask_token_id(self, vectorize_layer):\n","    mask_token_id = vectorize_layer([self.special_tokens]).numpy()[0][0]\n","    return mask_token_id\n","\n","  # Prepare model's input and output\n","  def process_data(self):\n","    # Encode all words\n","    encoded_texts = self.vectorize_layer(self.texts)\n","    encoded_texts = encoded_texts.numpy()\n","\n","    self.reset_random_seeds()\n","    # 15% BERT masking\n","    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n","    # Do not mask special tokens\n","    inp_mask[encoded_texts <= 2] = False\n","    # Set labels for masked tokens\n","    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n","    labels[inp_mask] = encoded_texts[inp_mask]\n","\n","    # Prepare input\n","    encoded_texts_masked = np.copy(encoded_texts)\n","    # Leave 10% masked token unchanged\n","    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n","    encoded_texts_masked[inp_mask_2mask] = self.mask_token_id\n","    # Set 10% masked token to a random token\n","    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n","    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n","        3, self.mask_token_id, inp_mask_2random.sum()\n","    )\n","\n","    # Prepare sample_weights to pass to .fit() method\n","    sample_weights = np.ones(labels.shape)\n","    sample_weights[labels == -1] = 0\n","\n","    # y_labels would be same as encoded_texts i.e input tokens\n","    y_labels = np.copy(encoded_texts)\n","\n","    return encoded_texts_masked, y_labels, sample_weights\n","\n","  # Create BERT (pretraining) model\n","  def bert_module(self, query, key, value, i):\n","    # Multi headed self-attention\n","    attention_output = layers.MultiHeadAttention(\n","        num_heads=self.num_head,\n","        key_dim=self.embed_dim // self.num_head,\n","        name=\"encoder_{}/multiheadattention\".format(i),\n","    )(query, key, value)\n","    attention_output = layers.Dropout(0.1, name=\"encoder_{}/att_dropout\".format(i))(\n","        attention_output\n","    )\n","    attention_output = layers.LayerNormalization(\n","        epsilon=1e-6, name=\"encoder_{}/att_layernormalization\".format(i)\n","    )(query + attention_output)\n","\n","    # Feed-forward layer\n","    ffn = keras.Sequential(\n","        [\n","            layers.Dense(self.ff_dim, activation=\"relu\"),\n","            layers.Dense(self.embed_dim),\n","        ],\n","        name=\"encoder_{}/ffn\".format(i),\n","    )\n","    ffn_output = ffn(attention_output)\n","    ffn_output = layers.Dropout(0.1, name=\"encoder_{}/ffn_dropout\".format(i))(\n","        ffn_output\n","    )\n","    sequence_output = layers.LayerNormalization(\n","        epsilon=1e-6, name=\"encoder_{}/ffn_layernormalization\".format(i)\n","    )(attention_output + ffn_output)\n","    return sequence_output\n","\n","  def get_pos_encoding_matrix(self, max_len, d_emb):\n","    pos_enc = np.array(\n","        [\n","            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n","            if pos != 0\n","            else np.zeros(d_emb)\n","            for pos in range(max_len)\n","        ]\n","    )\n","    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n","    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n","    return pos_enc\n","\n","  def create_masked_language_bert_model(self):\n","      inputs = layers.Input((self.max_len,), dtype=tf.int64)\n","\n","      word_embeddings = layers.Embedding(\n","          self.vocab_size, self.embed_dim, name=\"word_embedding\"\n","      )(inputs)\n","      position_embeddings = layers.Embedding(\n","          input_dim=self.max_len,\n","          output_dim=self.embed_dim,\n","          weights=[self.get_pos_encoding_matrix(self.max_len, self.embed_dim)],\n","          name=\"position_embedding\",\n","      )(tf.range(start=0, limit=self.max_len, delta=1))\n","      embeddings = word_embeddings + position_embeddings\n","\n","      encoder_output = embeddings\n","      for i in range(self.num_layers):\n","          encoder_output = self.bert_module(encoder_output, encoder_output, encoder_output, i)\n","\n","      mlm_output = layers.Dense(self.vocab_size, name=\"mlm_cls\", activation=\"softmax\")(\n","          encoder_output\n","      )\n","      mlm_model = MaskedLanguageModel(inputs, mlm_output, name=\"masked_bert_model\")\n","\n","      optimizer = keras.optimizers.Adam(learning_rate=self.lr)\n","      mlm_model.compile(optimizer=optimizer)\n","      return mlm_model\n","\n","  def train(self):\n","    # Initiate the model\n","    bert_masked_model = self.create_masked_language_bert_model()\n","\n","    # Prepare data for masked language model\n","    x_masked_train, y_masked_labels, sample_weights = self.process_data()\n","    mlm_ds = tf.data.Dataset.from_tensor_slices(\n","        (x_masked_train, y_masked_labels, sample_weights)\n","    )\n","    mlm_ds = mlm_ds.shuffle(5000).batch(self.batch_size)\n","\n","    # Fit the model\n","    bert_masked_model.fit(mlm_ds, epochs=1)\n","    return bert_masked_model\n","\n","  def bert_guess(self, clean_word, guessed_letters):\n","    vectorize_layer = self.vectorize_layer\n","    mask_token_id = self.mask_token_id\n","    bert_masked_model = self.train()\n","\n","    letters = list(set(string.ascii_lowercase) - set(guessed_letters))\n","\n","    try:\n","      sample_tokens = vectorize_layer([clean_word])\n","      prediction = bert_masked_model.predict(sample_tokens)\n","      masked_index = np.where(sample_tokens == mask_token_id)\n","      letter_probs = pd.DataFrame(prediction[0][masked_index], index=masked_index, columns=vectorize_layer.get_vocabulary())[letters]\n","      guess_letter = letter_probs.sum(axis=0).idxmax()\n","    except:\n","      guess_letter = '!'\n","\n","    return guess_letter"],"metadata":{"id":"rWeJ6JaSe1HR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# N-gram Model"],"metadata":{"id":"2KzJjpdKbJs4"}},{"cell_type":"code","source":["class NgramModel():\n","  def __init__(self, full_dictionary):\n","    self.full_dictionary = full_dictionary\n","    self.n_gram_max = 6\n","    self.full_dictionary_ngram_count = self.build_dictionary_ngram_count(self.full_dictionary)\n","    self.full_dictionary_by_length = self.build_dictionary_by_length(self.full_dictionary)\n","    self.full_dictionary_1gram_freq_by_length = self.build_dictionary_1gram_freq_by_length(self.full_dictionary_by_length)\n","\n","  def build_dictionary_ngram_count(self, dictionary):\n","    dict_ngram_count = collections.defaultdict(lambda: collections.defaultdict(float))\n","    for dict_word in dictionary:\n","      dict_word = '#' + dict_word + '#'\n","      for n in range(self.n_gram_max-1, -1, -1):\n","        for i in range(n, len(dict_word)):\n","          sliding_word = dict_word[i-n:i+1]\n","          if sliding_word == '#':\n","            continue\n","          dict_ngram_count[n][tuple([*sliding_word])] += 1\n","    return dict_ngram_count\n","\n","  def build_dictionary_by_length(self, full_dictionary):\n","    full_dictionary_by_length = collections.defaultdict(list)\n","    for dict_word in full_dictionary:\n","      full_dictionary_by_length[len(dict_word)].append(dict_word)\n","    return full_dictionary_by_length\n","\n","  def build_dictionary_1gram_freq_by_length(self, full_dictionary_by_length):\n","    full_dictionary_1gram_freq = collections.defaultdict(lambda: collections.defaultdict(float))\n","    for length in full_dictionary_by_length:\n","      full_dictionary_at_length = full_dictionary_by_length[length]\n","      letters_count_at_length = collections.Counter(\"\".join(full_dictionary_at_length))\n","      length_total_count = 0\n","      for letter in letters_count_at_length:\n","        length_total_count += letters_count_at_length[letter]\n","      for letter in letters_count_at_length:\n","        full_dictionary_1gram_freq[length][letter] = letters_count_at_length[letter] / length_total_count\n","    return full_dictionary_1gram_freq\n","\n","  def ngram_guess(self, clean_word, guessed_letters):\n","    n_gram_smoothing = [i/sum(range(1, self.n_gram_max+1)) for i in range(1, self.n_gram_max+1)]\n","    padded_word = '#' + clean_word + '#'\n","\n","    letter_probs = collections.defaultdict(float)\n","    letters = list(set(string.ascii_lowercase) - set(guessed_letters))\n","\n","    if clean_word.count('.') == len(clean_word):\n","      # 1gram frequency by length\n","      full_dictionary_1gram_freq_at_length = self.full_dictionary_1gram_freq_by_length[len(clean_word)]\n","      for letter in letters:\n","        letter_probs[letter] = full_dictionary_1gram_freq_at_length[letter]\n","    else:\n","      for n in range(self.n_gram_max-1, -1, -1):\n","        for i in range(n, len(padded_word)):\n","          sliding_word = padded_word[i-n:i+1]\n","          if sliding_word.count('.') == 1:\n","            total_count = 0\n","            count_by_letter = {}\n","            for letter in letters:\n","              sliding_word_potential = sliding_word.replace('.', letter)\n","              count = self.full_dictionary_ngram_count[n][tuple([*sliding_word_potential])]\n","              total_count += count\n","              count_by_letter[letter] = count\n","            if total_count > 0:\n","              for letter in letters:\n","                letter_probs[letter] += (count_by_letter[letter]/total_count) * n_gram_smoothing[n]\n","\n","    if sum(letter_probs.values()) == 0:\n","      guess_letter = '!'\n","    else:\n","      guess_letter = max(letter_probs, key=letter_probs.get)\n","    return guess_letter"],"metadata":{"id":"_uK-_G05bNQ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hangman Game"],"metadata":{"id":"0Qx6TRkLdvWL"}},{"cell_type":"code","execution_count":null,"metadata":{"jupyter":{"outputs_hidden":true},"id":"cwbXbzwstFwX"},"outputs":[],"source":["class Hangman():\n","  def __init__(self, full_dictionary):\n","    self.full_dictionary = full_dictionary\n","    self.guessed_letters = []\n","    self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n","    self.bert_model = BertModel(self.full_dictionary)\n","    self.ngram_model = NgramModel(self.full_dictionary)\n","\n","    def guess(self, word):\n","      word = word.lower()\n","      word = word.replace(\" \", \"\")\n","      clean_word = word.replace(\"_\",\".\")\n","\n","      # apply ngram algo to guess the letter of the highest probability\n","      guess_letter = self.ngram_model.ngram_guess(clean_word, self.guessed_letters)\n","\n","      # if word matching so far not found\n","      if guess_letter == '!':\n","        sorted_letter_count = self.full_dictionary_common_letter_sorted\n","        for letter,instance_count in sorted_letter_count:\n","          if letter not in self.guessed_letters:\n","            guess_letter = letter\n","            break\n","\n","      return guess_letter\n","\n","    def start_game(self, test_words):\n","      success = 0\n","      total_trials = len(test_words)\n","      word_fails = []\n","      for word in test_words[:total_trials]:\n","        self.guessed_letters = []\n","        word = word.replace(\" \", \"\")\n","        mask = ['_'] * len(word)\n","        masked_word = '_' * len(word)\n","\n","        # start tries\n","        tries_remains = 6\n","        while tries_remains>0:\n","            print('tries_remains: ', tries_remains)\n","            # get guessed letter from user code\n","            guess_letter = self.guess(masked_word)\n","            print('Guess is ', guess_letter)\n","\n","            # append guessed letter to guessed letters field in hangman object\n","            self.guessed_letters.append(guess_letter)\n","            if guess_letter in word and len(guess_letter) == 1:\n","              for i, c in enumerate(word):\n","                if c == guess_letter:\n","                  mask[i] = c\n","              masked_word = ''.join(mask)\n","              print('Correct! Word Update: ', mask)\n","              if '_' not in masked_word:\n","                success += 1\n","                print('!succeed!')\n","                break\n","            else:\n","              tries_remains -= 1\n","        if tries_remains == 0: # fails\n","          word_fails.append(word)\n","          print('!fail!')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.15"},"colab":{"provenance":[{"file_id":"1rm7-x3dv2kYrXZJ-My0Xv80KU_scE2Wl","timestamp":1705796669806}]}},"nbformat":4,"nbformat_minor":0}